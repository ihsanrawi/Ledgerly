# Test Design: Story 2.3 - Automatic Column Detection

**Date:** 2025-10-15
**Designer:** Quinn (Test Architect)
**Story ID:** 2.3
**Story Title:** Automatic Column Detection

---

## Test Strategy Overview

- **Total test scenarios:** 47
- **Unit tests:** 28 (60%)
- **Integration tests:** 15 (32%)
- **E2E tests:** 4 (8%)
- **Priority distribution:** P0: 19, P1: 18, P2: 10

**Test Philosophy:** Shift-left approach with emphasis on unit-level detection algorithm validation. Integration tests verify API contract and CSV sample accuracy. E2E tests validate critical user journey with confidence indicators.

**Risk-Based Focus:** Concentrated coverage on TECH-001 (detection accuracy <90%) and DATA-001 (incorrect column mapping) identified in risk profile.

---

## Test Scenarios by Acceptance Criteria

### AC1: Heuristic algorithm detects date column with >90% accuracy

**Target:** Pattern matching for MM/DD/YYYY, YYYY-MM-DD, DD.MM.YYYY date formats

#### Unit Test Scenarios (ColumnDetectionService)

| ID | Level | Priority | Test Scenario | Justification | Mitigates Risk |
|---|---|---|---|---|---|
| 2.3-UNIT-001 | Unit | P0 | Detect date column with exact "Date" header | Baseline detection for most common header | TECH-001 |
| 2.3-UNIT-002 | Unit | P0 | Detect date column with "Transaction Date" header | Second most common variant (25% of samples) | TECH-001 |
| 2.3-UNIT-003 | Unit | P0 | Detect date column with "Post Date" header | Third common variant (17% of samples) | TECH-001 |
| 2.3-UNIT-004 | Unit | P1 | Detect date column with German "Datum" header | Multi-language support requirement | TECH-001 |
| 2.3-UNIT-005 | Unit | P1 | Detect date column with case variation "transaction date" | Case-insensitive matching requirement | TECH-001 |
| 2.3-UNIT-006 | Unit | P0 | Validate date format MM/DD/YYYY parsing (01/15/2025) | Most common format (50% of samples) | TECH-001 |
| 2.3-UNIT-007 | Unit | P0 | Validate date format YYYY-MM-DD parsing (2025-01-15) | ISO format (33% of samples) | TECH-001 |
| 2.3-UNIT-008 | Unit | P0 | Validate date format DD.MM.YYYY parsing (15.01.2025) | European format (17% of samples) | TECH-001 |
| 2.3-UNIT-009 | Unit | P1 | Validate non-zero-padded date format (1/5/2025) | Edge case from Chase bank samples | TECH-001 |
| 2.3-UNIT-010 | Unit | P0 | Calculate high confidence (>0.9) when exact header match + 100% sample row validation | Confidence scoring accuracy | DATA-001 |
| 2.3-UNIT-011 | Unit | P0 | Calculate medium confidence (0.7-0.9) when fuzzy match + >70% validation | Threshold boundary condition | DATA-001 |
| 2.3-UNIT-012 | Unit | P0 | Calculate low confidence (<0.7) when weak match or failed validation | Must trigger manual review UI | DATA-001 |
| 2.3-UNIT-013 | Unit | P1 | Handle no date column found gracefully (return empty, not exception) | Error handling requirement | OPS-001 |
| 2.3-UNIT-014 | Unit | P2 | Detect date column with "Effective Date" variant | Less common variant | TECH-001 |

**Rationale for unit-level testing:** Date detection is pure algorithmic logic with pattern matching. Fast feedback loop required given >90% accuracy requirement. No database or external dependencies needed.

---

#### Integration Test Scenarios (PreviewCsvHandler)

| ID | Level | Priority | Test Scenario | Justification | Mitigates Risk |
|---|---|---|---|---|---|
| 2.3-INT-001 | Integration | P0 | Detect date column in all 4 standard CSV samples | Validate real-world accuracy with standard formats | TECH-001 |
| 2.3-INT-002 | Integration | P0 | Detect date column in all 4 edge case CSV samples | Validate robustness with multi-line memos, special chars | TECH-001, TECH-002 |
| 2.3-INT-003 | Integration | P1 | Handle 4 malformed CSV samples gracefully (low confidence expected) | Error handling with invalid data | OPS-001 |
| 2.3-INT-004 | Integration | P1 | Detect date column in no-headers.csv via data pattern analysis | Edge case: infer from row 1 data instead of headers | TECH-003 |

**Rationale for integration-level testing:** Requires full CSV parsing pipeline (CsvParserService → ColumnDetectionService → PreviewCsvResponse). Validates detection works with real CSV file formats.

---

### AC2: Amount column detected (numeric values, optional currency symbols, decimal separators)

**Target:** Detect single amount column OR debit/credit split columns with proper numeric parsing

#### Unit Test Scenarios (ColumnDetectionService)

| ID | Level | Priority | Test Scenario | Justification | Mitigates Risk |
|---|---|---|---|---|---|
| 2.3-UNIT-015 | Unit | P0 | Detect single "Amount" column with positive/negative values | Most common pattern (50% of samples) | TECH-001 |
| 2.3-UNIT-016 | Unit | P0 | Detect debit/credit split columns ("Debit" + "Credit" headers) | Second pattern (42% of samples) | TECH-001 |
| 2.3-UNIT-017 | Unit | P0 | Parse negative amounts with minus prefix (-50.00) | Most common negative format (75% of samples) | DATA-001 |
| 2.3-UNIT-018 | Unit | P0 | Parse negative amounts with parentheses (50.00) → -50.00 | Credit card format (25% of samples) | DATA-001, TECH-001 |
| 2.3-UNIT-019 | Unit | P0 | Parse European decimal separator (1.234,56 → 1234.56) | European format (17% of samples) | DATA-002 |
| 2.3-UNIT-020 | Unit | P1 | Parse US thousands separator (1,234.56 → 1234.56) | US format with grouping | DATA-001 |
| 2.3-UNIT-021 | Unit | P1 | Strip currency symbols ($50.00, €50.00) before parsing | Optional currency symbol handling | DATA-001 |
| 2.3-UNIT-022 | Unit | P1 | Detect German "Betrag" amount column header | Multi-language support | TECH-001 |
| 2.3-UNIT-023 | Unit | P0 | Calculate confidence based on numeric validation of sample rows | Non-numeric values should lower confidence | DATA-001 |
| 2.3-UNIT-024 | Unit | P1 | Handle empty cells in debit/credit split (debit=50.00, credit=empty) | Common pattern in split columns | DATA-001 |
| 2.3-UNIT-025 | Unit | P2 | Detect amount column with "Transaction Amount" variant | Less common header variant | TECH-001 |

**Rationale for unit-level testing:** Amount parsing is complex algorithmic logic (parentheses, European format, currency stripping). Critical for data integrity. Fast unit tests provide immediate feedback on edge cases.

---

#### Integration Test Scenarios

| ID | Level | Priority | Test Scenario | Justification | Mitigates Risk |
|---|---|---|---|---|---|
| 2.3-INT-005 | Integration | P0 | Detect amount column in standard CSV samples (single amount) | Validate with real-world data | TECH-001 |
| 2.3-INT-006 | Integration | P0 | Detect debit/credit split in samples with separate columns | Validate split column detection | TECH-001 |
| 2.3-INT-007 | Integration | P1 | Detect amount in European CSV with semicolon delimiter + comma decimal | Combined edge case: delimiter + number format | DATA-002 |

**Rationale for integration-level testing:** Amount detection depends on CSV parsing results. Validates end-to-end flow with actual file formats.

---

### AC3: Payee/description column detected (text values, typically longest non-numeric column)

**Target:** Identify primary description/payee field using header matching and heuristics

#### Unit Test Scenarios (ColumnDetectionService)

| ID | Level | Priority | Test Scenario | Justification | Mitigates Risk |
|---|---|---|---|---|---|
| 2.3-UNIT-026 | Unit | P0 | Detect description column with "Description" header | Most common pattern (67% of samples) | TECH-001 |
| 2.3-UNIT-027 | Unit | P1 | Detect description column with "Payee" header | Common variant (8% of samples) | TECH-001 |
| 2.3-UNIT-028 | Unit | P1 | Detect description column with "Merchant" header | Alternative variant | TECH-001 |
| 2.3-UNIT-029 | Unit | P1 | Detect description with German "Beschreibung" header | Multi-language support | TECH-001 |
| 2.3-UNIT-030 | Unit | P2 | Detect description as "longest non-numeric column" when header ambiguous | Heuristic fallback when header unclear | TECH-001 |
| 2.3-UNIT-031 | Unit | P1 | Handle special characters in description (accents, emoji, ampersand) | Edge case: special-characters.csv sample | TECH-002 |

**Rationale for unit-level testing:** Description detection uses fuzzy matching logic and heuristics. Pure logic test without dependencies.

---

#### Integration Test Scenarios

| ID | Level | Priority | Test Scenario | Justification | Mitigates Risk |
|---|---|---|---|---|---|
| 2.3-INT-008 | Integration | P0 | Detect description column in all 12 CSV samples | Validate header matching across all samples | TECH-001 |
| 2.3-INT-009 | Integration | P1 | Detect description in special-characters.csv (emoji, accents) | UTF-8 encoding + special char handling | TECH-002 |

**Rationale for integration-level testing:** Validates detection works with full CSV parsing, including encoding handling.

---

### AC4: Memo column detected (optional, secondary text column)

**Target:** Identify secondary text column when present (not required field)

#### Unit Test Scenarios (ColumnDetectionService)

| ID | Level | Priority | Test Scenario | Justification | Mitigates Risk |
|---|---|---|---|---|---|
| 2.3-UNIT-032 | Unit | P1 | Detect memo column when "Memo" header present | Optional field detection | TECH-001 |
| 2.3-UNIT-033 | Unit | P2 | Detect secondary text column as memo when header ambiguous | Heuristic: second-longest text column | TECH-001 |
| 2.3-UNIT-034 | Unit | P1 | Return null/empty for memo when only one text column present | No false positives when memo absent | DATA-001 |
| 2.3-UNIT-035 | Unit | P1 | Handle multi-line memo field (quoted with newlines) | Edge case: multiline-memo.csv sample | TECH-002 |

**Rationale for unit-level testing:** Memo detection is algorithmic logic. Optional field means lower priority than date/amount.

---

#### Integration Test Scenarios

| ID | Level | Priority | Test Scenario | Justification | Mitigates Risk |
|---|---|---|---|---|---|
| 2.3-INT-010 | Integration | P1 | Detect memo column in samples that include memo field | Validate optional field detection | TECH-001 |
| 2.3-INT-011 | Integration | P2 | Verify no memo detected in samples without memo field | Prevent false positive detection | DATA-001 |

**Rationale for integration-level testing:** Validates memo detection doesn't interfere with required field detection.

---

### AC5: Confidence indicators displayed (Green checkmark, yellow warning, red alert)

**Target:** UI displays confidence scores with color-coded icons

#### Unit Test Scenarios (import-csv.component.spec.ts)

| ID | Level | Priority | Test Scenario | Justification | Mitigates Risk |
|---|---|---|---|---|---|
| 2.3-UNIT-036 | Unit | P0 | Render green checkmark icon when confidence >0.9 | High confidence indicator | BUS-001 |
| 2.3-UNIT-037 | Unit | P0 | Render yellow warning icon when confidence 0.7-0.9 | Medium confidence indicator | BUS-001 |
| 2.3-UNIT-038 | Unit | P0 | Render red alert icon when confidence <0.7 | Low confidence indicator (triggers review) | BUS-001, DATA-001 |
| 2.3-UNIT-039 | Unit | P0 | Display field type badge on column header (Date, Amount, Description, Memo) | Field identification in UI | BUS-001 |
| 2.3-UNIT-040 | Unit | P1 | Display confidence score in tooltip on icon hover | Educational: show numeric score | BUS-001 |
| 2.3-UNIT-041 | Unit | P0 | Display warning banner when any required field confidence <0.7 | Alert user to review mapping | BUS-001, DATA-001 |
| 2.3-UNIT-042 | Unit | P0 | Disable "Next" button when required fields (date, amount) confidence <0.7 | Prevent bad data from proceeding | DATA-001 |
| 2.3-UNIT-043 | Unit | P1 | Display helpful message: "Please review column mapping before proceeding" | User guidance on low confidence | BUS-001 |

**Rationale for unit-level testing:** Component rendering logic is isolated. Fast tests for UI state management. No backend dependencies needed.

---

#### E2E Test Scenarios (Playwright/Cypress)

| ID | Level | Priority | Test Scenario | Justification | Mitigates Risk |
|---|---|---|---|---|---|
| 2.3-E2E-001 | E2E | P0 | Upload standard CSV → verify green checkmarks on all required columns → Next enabled | Critical happy path: successful detection UX | BUS-001 |
| 2.3-E2E-002 | E2E | P0 | Upload ambiguous CSV → verify yellow/red warnings → verify Next disabled → verify banner message | Critical unhappy path: low confidence UX | BUS-001, DATA-001 |
| 2.3-E2E-003 | E2E | P1 | Hover over confidence icon → verify tooltip displays numeric score | User education flow | BUS-001 |
| 2.3-E2E-004 | E2E | P2 | Upload CSV with optional memo detected → verify memo badge displayed | Optional field detection UX | TECH-001 |

**Rationale for E2E-level testing:** Validates complete user journey from upload to detection results display. Critical for BUS-001 (user trust) risk mitigation. Visual validation requires browser automation.

---

### AC6: Integration test validates detection for 15+ CSV formats with >90% success rate

**Target:** Comprehensive validation against Story 2.1 test samples

#### Integration Test Scenarios

| ID | Level | Priority | Test Scenario | Justification | Mitigates Risk |
|---|---|---|---|---|---|
| 2.3-INT-012 | Integration | P0 | Upload all 4 standard CSV samples → assert 4/4 successful detection (date + amount confidence >0.9) | AC6 success criteria: ≥90% for standard samples | TECH-001 |
| 2.3-INT-013 | Integration | P0 | Upload all 4 edge case CSV samples → assert 4/4 successful detection (confidence >0.7) | AC6 success criteria: ≥90% for edge cases | TECH-001, TECH-002 |
| 2.3-INT-014 | Integration | P1 | Upload all 4 malformed CSV samples → assert low confidence or detection failure (expected) | Verify graceful failure handling | OPS-001 |
| 2.3-INT-015 | Integration | P0 | Generate test report documenting detection results for all 12 samples | Evidence for story completion | TECH-001 |

**Rationale for integration-level testing:** AC6 explicitly requires integration test. Validates detection accuracy against real-world samples. This is the definitive test for story acceptance.

---

## Risk Coverage Matrix

**Mapping test scenarios to identified risks from risk profile:**

| Risk ID | Risk Title | Test Coverage | Tests |
|---------|------------|---------------|-------|
| **TECH-001** (Critical) | Detection accuracy <90% | **28 tests** | UNIT-001 to UNIT-014, UNIT-015 to UNIT-035, INT-001 to INT-015 |
| **DATA-001** (High) | Incorrect column mapping | **15 tests** | UNIT-010 to UNIT-012, UNIT-017 to UNIT-024, UNIT-042, INT-006, E2E-002 |
| **PERF-001** (High) | Detection timeout | **1 test** | Performance test (separate section below) |
| **BUS-001** (High) | Poor UX damages trust | **12 tests** | UNIT-036 to UNIT-043, E2E-001 to E2E-004 |
| **TECH-002** (Medium) | Multi-line memo detection | **4 tests** | UNIT-031, UNIT-035, INT-002, INT-009 |
| **TECH-003** (Medium) | No header row detection | **2 tests** | UNIT-013, INT-004 |
| **DATA-002** (Medium) | European number format | **2 tests** | UNIT-019, INT-007 |
| **SEC-001** (Medium) | CSV content logged | **1 test** | Security audit (separate section below) |
| **OPS-001** (Medium) | Detection service failure | **3 tests** | UNIT-013, INT-003, INT-014 |

**Coverage Assessment:** All critical and high risks have comprehensive test coverage. Medium risks have targeted coverage. Low risks accepted without dedicated tests.

---

## Performance Testing

### Performance Test Scenarios

| ID | Level | Priority | Test Scenario | Target | Mitigates Risk |
|---|---|---|---|---|---|
| 2.3-PERF-001 | Integration | P0 | Measure detection time for 1,000-row CSV file | <100ms | PERF-001 |
| 2.3-PERF-002 | Integration | P1 | Profile detection algorithm with dotnet-trace | Identify bottlenecks | PERF-001 |
| 2.3-PERF-003 | Integration | P1 | Load test: 100 concurrent preview requests | Verify no resource contention | PERF-001 |
| 2.3-PERF-004 | Integration | P2 | Verify column-name-mapping.json loaded once at startup (not per request) | Optimize initialization | PERF-002 |

**Performance Targets:**
- Detection time: <100ms (P95)
- Total preview endpoint: <2s for 1,000-row CSV
- Concurrent requests: 100 users without degradation

---

## Security Testing

### Security Test Scenarios

| ID | Level | Priority | Test Scenario | Mitigates Risk |
|---|---|---|---|---|
| 2.3-SEC-001 | Code Review | P0 | Audit all log statements in ColumnDetectionService → verify no CSV content logged | SEC-001 |
| 2.3-SEC-002 | Integration | P1 | Verify only metadata logged (header names, field types, confidence scores) | SEC-001 |
| 2.3-SEC-003 | Integration | P1 | Verify error messages sanitized (no user data in exception messages) | SEC-001 |

**Security Requirements:**
- **NEVER log:** CSV sample rows, payee names, amounts, dates (contains PII)
- **Safe to log:** Header names, field type mappings, confidence scores, file metadata

---

## Test Data Requirements

### CSV Test Samples (from Story 2.1)

**Standard Samples (4 files):**
1. `chase-checking.csv` - US format, comma delimiter, MM/DD/YYYY
2. `wells-fargo-checking.csv` - ISO format, comma delimiter, YYYY-MM-DD
3. `bofa-checking.csv` - US format, comma delimiter, debit/credit split
4. `citi-credit.csv` - US format, parentheses negatives

**Edge Case Samples (4 files):**
5. `european-bank.csv` - Semicolon delimiter, DD.MM.YYYY, German headers
6. `multiline-memo.csv` - Quoted multi-line memo field
7. `special-characters.csv` - Emoji, accents, ampersands
8. `no-headers.csv` - No header row, infer from data

**Malformed Samples (4 files):**
9. `inconsistent-columns.csv` - Row length varies
10. `invalid-dates.csv` - Non-date values in date column
11. `missing-required.csv` - No date or amount column
12. `encoding-issues.csv` - Mixed encoding (intentional corruption)

**Reference Data:**
- `column-name-mapping.json` - Header variant patterns
- `CSV_FORMAT_ANALYSIS.md` - Format documentation

---

## Test Execution Strategy

### Recommended Execution Order

**Phase 1: Fail-Fast Critical Path (P0 Unit Tests)**
1. Run P0 unit tests for date detection (UNIT-001 to UNIT-014)
2. Run P0 unit tests for amount detection (UNIT-015 to UNIT-024)
3. Run P0 unit tests for confidence scoring (UNIT-010 to UNIT-012)
4. Run P0 UI unit tests (UNIT-036 to UNIT-043)
5. **Exit criteria:** 100% pass rate, <10s execution time

**Phase 2: Integration Validation (P0 Integration Tests)**
1. Run P0 integration tests with all CSV samples (INT-001 to INT-015)
2. Validate ≥90% accuracy (8/8 standard + edge cases must pass)
3. **Exit criteria:** AC6 validated, test report generated

**Phase 3: E2E User Journey (P0 E2E Tests)**
1. Run E2E-001 (happy path: successful detection)
2. Run E2E-002 (unhappy path: low confidence handling)
3. **Exit criteria:** Critical UX flows validated

**Phase 4: Performance & Security (P0 Non-Functional)**
1. Run PERF-001 (detection time <100ms)
2. Run SEC-001 (security audit)
3. **Exit criteria:** Performance targets met, no PII logged

**Phase 5: P1/P2 Scenarios (Time Permitting)**
1. Run P1 unit tests (edge cases, multi-language)
2. Run P1 integration tests (European format, no headers)
3. Run P2 tests (nice-to-have scenarios)

---

## Coverage Validation

### AC Coverage Checklist

- [x] **AC1:** Date column detection - **14 unit + 4 integration** = 18 tests
- [x] **AC2:** Amount column detection - **11 unit + 3 integration** = 14 tests
- [x] **AC3:** Description column detection - **6 unit + 2 integration** = 8 tests
- [x] **AC4:** Memo column detection - **4 unit + 2 integration** = 6 tests
- [x] **AC5:** Confidence indicators UI - **8 unit + 4 E2E** = 12 tests
- [x] **AC6:** Integration test validation - **4 integration** = 4 tests

**Total:** 62 test scenarios (47 functional + 7 performance + 3 security + 5 miscellaneous)

### Test Level Distribution Validation

| Test Level | Count | Percentage | Guideline | Status |
|------------|-------|------------|-----------|--------|
| Unit | 43 | 68% | >60% | ✅ PASS |
| Integration | 15 | 24% | 20-30% | ✅ PASS |
| E2E | 4 | 6% | <10% | ✅ PASS |
| Other (Perf/Sec) | 5 | 2% | - | - |

**Assessment:** Test pyramid structure maintained. Proper shift-left strategy with unit tests dominant.

### Priority Distribution Validation

| Priority | Count | Percentage | Expected |
|----------|-------|------------|----------|
| P0 | 19 | 40% | 30-50% (critical paths) |
| P1 | 18 | 38% | 30-40% (core features) |
| P2 | 10 | 22% | 20-30% (nice-to-have) |

**Assessment:** Balanced priority distribution. P0 tests cover all critical paths and risks.

---

## Test Implementation Guidance

### Unit Test Structure (AAA Pattern)

**Example: ColumnDetectionServiceTests.cs**

```csharp
[Fact]
public async Task DetectDateColumn_WithExactDateHeader_ReturnsHighConfidence()
{
    // Arrange
    var headers = new[] { "Date", "Description", "Amount" };
    var sampleRows = new List<Dictionary<string, string>>
    {
        new() { ["Date"] = "01/15/2025", ["Description"] = "Purchase", ["Amount"] = "-50.00" },
        new() { ["Date"] = "01/14/2025", ["Description"] = "Deposit", ["Amount"] = "100.00" }
    };
    var service = new ColumnDetectionService(_logger, _columnMappingConfig);

    // Act
    var result = await service.DetectColumns(headers, sampleRows);

    // Assert
    Assert.True(result.DetectedMappings.ContainsKey("Date"));
    Assert.Equal("date", result.DetectedMappings["Date"]);
    Assert.True(result.ConfidenceScores["date"] > 0.9);
}
```

### Integration Test Structure

**Example: PreviewCsvHandlerTests.cs**

```csharp
[Fact]
public async Task PreviewCsv_WithStandardChaseSample_DetectsAllColumnsCorrectly()
{
    // Arrange
    var csvFilePath = "tests/TestData/CsvSamples/standard/chase-checking.csv";
    var command = new PreviewCsvCommand { FilePath = csvFilePath };

    // Act
    var response = await _handler.Handle(command, CancellationToken.None);

    // Assert
    Assert.NotNull(response.ColumnDetection);
    Assert.True(response.ColumnDetection.AllRequiredFieldsDetected);
    Assert.True(response.ColumnDetection.ConfidenceScores["date"] > 0.9);
    Assert.True(response.ColumnDetection.ConfidenceScores["amount"] > 0.9);
    Assert.Empty(response.ColumnDetection.Warnings);
}
```

### E2E Test Structure (Playwright)

**Example: import-csv.e2e.spec.ts**

```typescript
test('Upload standard CSV and verify confidence indicators', async ({ page }) => {
  // Arrange
  await page.goto('/import');

  // Act
  await page.setInputFiles('input[type="file"]', 'tests/fixtures/chase-checking.csv');
  await page.waitForSelector('.preview-table');

  // Assert
  const dateIcon = page.locator('th:has-text("Date") .confidence-icon');
  await expect(dateIcon).toHaveClass(/check-circle/); // Green checkmark

  const amountIcon = page.locator('th:has-text("Amount") .confidence-icon');
  await expect(amountIcon).toHaveClass(/check-circle/);

  const nextButton = page.locator('button:has-text("Next")');
  await expect(nextButton).toBeEnabled();
});
```

---

## Test Maintenance Considerations

### Test Stability

**High stability tests (Unit):**
- Detection algorithm logic unlikely to change
- Fast execution, no flakiness expected
- Minimal maintenance burden

**Medium stability tests (Integration):**
- Dependent on CSV sample files (static)
- May need updates if detection logic changes
- Moderate maintenance burden

**Low stability tests (E2E):**
- UI changes will break tests
- Selector changes require updates
- Highest maintenance burden - keep minimal

### Test Data Management

**CSV samples:** Store in `tests/TestData/CsvSamples/` (version controlled)
**Reference data:** `column-name-mapping.json` (update when new patterns added)
**Test isolation:** Each test uses independent CSV file (no shared state)

---

## Success Criteria

### Story Complete When:

1. ✅ **All P0 tests pass** (19 scenarios)
2. ✅ **AC6 integration test demonstrates ≥90% accuracy** (8/8 standard + edge cases)
3. ✅ **Performance tests meet targets** (<100ms detection time)
4. ✅ **Security audit confirms no PII logging**
5. ✅ **Test coverage meets minimums:**
   - Backend unit coverage: ≥80%
   - Frontend unit coverage: ≥70%
   - Integration coverage: All 12 CSV samples tested

### Quality Gate Integration

**Test execution results feed into quality gate decision:**

- **P0 tests:** Any failure → Gate = FAIL
- **Integration test accuracy <90%:** Gate = FAIL (blocks AC6)
- **Performance tests fail:** Gate = CONCERNS
- **P1 tests fail:** Gate = CONCERNS
- **All P0+P1 pass, performance OK:** Gate = PASS

---

## Appendix: Test Scenario Summary Table

### Quick Reference

| Category | Unit | Integration | E2E | Total | Priority |
|----------|------|-------------|-----|-------|----------|
| Date Detection | 14 | 4 | - | 18 | P0: 10, P1: 6, P2: 2 |
| Amount Detection | 11 | 3 | - | 14 | P0: 9, P1: 4, P2: 1 |
| Description Detection | 6 | 2 | - | 8 | P0: 1, P1: 4, P2: 3 |
| Memo Detection | 4 | 2 | - | 6 | P1: 3, P2: 3 |
| Confidence UI | 8 | - | 4 | 12 | P0: 8, P1: 2, P2: 2 |
| CSV Sample Validation | - | 4 | - | 4 | P0: 3, P1: 1 |
| Performance | - | 4 | - | 4 | P0: 1, P1: 2, P2: 1 |
| Security | - | 3 | - | 3 | P0: 1, P1: 2 |
| **TOTAL** | **43** | **22** | **4** | **69** | **P0: 33, P1: 24, P2: 12** |

*(Note: Adjusted total reflects all scenarios including performance and security tests)*

---

**End of Test Design**

*Designed by Quinn (Test Architect) - 2025-10-15*
