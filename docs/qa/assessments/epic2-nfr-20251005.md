# NFR Assessment: Epic 2 - Import CSV Vertical Slice

**Date:** 2025-10-05
**Reviewer:** Quinn (QA Test Architect)
**Epic:** Epic 2: CSV Import & Smart Data Entry
**Scope:** Security, Performance, Reliability, Maintainability

---

## Executive Summary

| NFR Area | Status | Quality Score Impact |
|----------|--------|---------------------|
| **Security** | CONCERNS | -10 |
| **Performance** | CONCERNS | -10 |
| **Reliability** | CONCERNS | -10 |
| **Maintainability** | PASS | 0 |

**Overall Quality Score:** 70/100

**Risk Level:** ⚠️ **HIGH** - This is the HIGHEST RISK EPIC in the project

---

## Security Assessment

**Status:** CONCERNS

### Findings

✅ **PASS Areas:**
- Input validation using FluentValidation at API boundary
- No hardcoded secrets
- File system permissions checked before .hledger access

⚠️ **CONCERNS:**
1. **CSV Injection Vulnerability**
   - Risk: Malicious CSV with formulas (=cmd|'/c calc') could execute on export (Story 2.6)
   - Attack Vector: User imports malicious CSV → app stores formula → exports to Excel → RCE
   - Mitigation: Sanitize CSV cells starting with `=`, `+`, `-`, `@`, `|` before storage
   - Impact: **CRITICAL** (Remote Code Execution via export)

2. **Path Traversal in CSV Upload**
   - Risk: Crafted filename (../../etc/passwd.csv) could access unintended files
   - Mitigation: Validate filename, use GUID for temp storage (Story 2.2)
   - Impact: HIGH (File system access)

3. **Encoding-Based Injection**
   - Risk: ISO-8859-1 → UTF-8 conversion edge cases (Story 2.2 AC #3)
   - Attack Vector: NULL bytes, BOMs, homoglyphs in payee names
   - Mitigation: Sanitize after encoding detection, reject invalid UTF-8
   - Impact: MEDIUM (Data integrity)

4. **Unlimited File Size**
   - Risk: DoS via massive CSV upload (Story 2.2)
   - Mitigation: Enforce file size limit (recommend 10MB max for MVP)
   - Impact: MEDIUM (Availability)

### Recommendations

**Critical (Must-Fix Before Release):**
1. Add CSV injection sanitization (Story 2.6):
   ```csharp
   public string SanitizeCsvCell(string value)
   {
       if (value.StartsWith("=") || value.StartsWith("+") ||
           value.StartsWith("-") || value.StartsWith("@") || value.StartsWith("|"))
       {
           return "'" + value; // Prefix with single quote to force text
       }
       return value;
   }
   ```
   **Effort:** 3 hours (includes tests)

2. Add path traversal protection (Story 2.2):
   ```csharp
   var safeFileName = $"{Guid.NewGuid()}.csv";
   var uploadPath = Path.Combine(_tempDir, safeFileName);
   ```
   **Effort:** 1 hour

3. Add file size limit validation (Story 2.2):
   ```csharp
   if (fileSize > 10 * 1024 * 1024) // 10MB
       throw new ValidationException("CSV file exceeds 10MB limit");
   ```
   **Effort:** 30 minutes

**Important (Add in Epic 3):**
- Add encoding validation test for NULL byte, BOM, homoglyph attacks

---

## Performance Assessment

**Status:** CONCERNS

### Findings

✅ **PASS Areas:**
- NFR2 target defined: <5s for 1,000 transactions
- Progress indicator for large files (Story 2.2 AC #5)

⚠️ **CONCERNS:**
1. **No Performance Test in Epic 2**
   - Risk: NFR2 (<5s for 1,000 txns) not validated until Epic 3 Story 3.6
   - Mitigation: Add performance test to Story 2.6
   - Impact: HIGH (Could discover issues late)

2. **Duplicate Detection O(n²) Complexity**
   - Risk: Hashing all imported txns against all existing txns = O(n²) (Story 2.5)
   - Calculation: 1,000 imported × 5,000 existing = 5M comparisons
   - Mitigation: Use hash index lookup (O(n) with dictionary/SQLite index)
   - Impact: HIGH (Could exceed 5s target)

3. **CSV Parsing Blocks UI Thread**
   - Risk: CsvHelper sync parsing freezes UI for large files (Story 2.2)
   - Mitigation: Use async streaming parser + chunked processing
   - Impact: MEDIUM (UX degradation, violates NFR5: 60fps)

4. **Category Suggestion N+1 Query**
   - Risk: Story 2.5 AC #3 queries ImportRules per transaction (1,000 queries for 1,000 txns)
   - Mitigation: Preload all ImportRules into memory dictionary
   - Impact: MEDIUM (Could add 2-3s to import time)

### Recommendations

**Critical (Fix in Epic 2):**
1. Add performance test to Story 2.6:
   ```csharp
   [Fact]
   public async Task ImportCsv_1000Transactions_CompletesUnder5Seconds()
   {
       var csv = Generate1000TransactionCsv();
       var stopwatch = Stopwatch.StartNew();
       await _importHandler.Handle(new ImportCsvCommand(csv));
       stopwatch.Stop();
       Assert.True(stopwatch.Elapsed < TimeSpan.FromSeconds(5));
   }
   ```
   **Effort:** 4 hours (includes test data generation)

2. Use hash index for duplicate detection (Story 2.5):
   ```csharp
   var existingHashes = await _dbContext.Transactions
       .Select(t => t.Hash)
       .ToDictionaryAsync(h => h, h => true); // O(1) lookup
   ```
   **Effort:** 2 hours

3. Use async streaming CSV parser (Story 2.2):
   ```csharp
   await foreach (var record in csvReader.GetRecordsAsync<CsvRow>())
   {
       // Process in chunks of 100, yield to UI thread
       if (++count % 100 == 0)
           await Task.Yield();
   }
   ```
   **Effort:** 3 hours

**Important (Fix in Epic 2):**
4. Preload ImportRules dictionary (Story 2.5):
   ```csharp
   var rules = await _dbContext.ImportRules.ToListAsync();
   var ruleDict = rules.ToDictionary(r => r.PayeePattern, r => r.Category);
   ```
   **Effort:** 1 hour

---

## Reliability Assessment

**Status:** CONCERNS

### Findings

✅ **PASS Areas:**
- Atomic .hledger file append (Story 2.2 AC #7 implies Epic 1's atomic writes)
- .bak backup before write (inherited from Epic 1)
- Validation with `hledger check` before commit

⚠️ **CONCERNS:**
1. **Partial Import Failure Recovery**
   - Risk: Import fails at transaction 500/1000 → what happens to first 499?
   - Current: Story 2.6 doesn't specify rollback strategy
   - Mitigation: Use database transaction for all-or-nothing import
   - Impact: HIGH (Data consistency)

2. **Duplicate Hash Collision**
   - Risk: Different txns with same date+payee+amount create identical hashes (Story 2.5 AC #1)
   - Example: Two $50 Amazon purchases on same day → hash collision → false duplicate warning
   - Mitigation: Add time-of-day or memo to hash, or allow "Import Anyway" (AC #2 provides this)
   - Impact: MEDIUM (False positives annoy users)

3. **Malformed CSV Recovery**
   - Risk: Story 2.2 AC #4 shows error but doesn't specify recovery action
   - Mitigation: Offer "skip bad rows" vs "abort import" options
   - Impact: MEDIUM (UX issue, not data loss)

4. **Import History Not Used for Recovery**
   - Risk: Story 2.6 AC #6 logs import history but doesn't enable "undo last import"
   - Mitigation: Add "Undo Import" button (deferred to Phase 2 acceptable)
   - Impact: LOW (Workaround: manual deletion)

### Recommendations

**Critical (Fix in Epic 2):**
1. Add transactional import (Story 2.6):
   ```csharp
   using var dbTxn = await _dbContext.Database.BeginTransactionAsync();
   try
   {
       foreach (var txn in transactions)
           await _importHandler.Handle(new AddTransactionCommand(txn));
       await dbTxn.CommitAsync();
   }
   catch
   {
       await dbTxn.RollbackAsync();
       await _hledgerWriter.RestoreFromBackup(); // Revert .hledger
       throw;
   }
   ```
   **Effort:** 4 hours (includes rollback tests)

2. Improve hash uniqueness (Story 2.5):
   ```csharp
   var hash = SHA256($"{date:yyyy-MM-dd}|{payee}|{amount}|{memo.Substring(0,20)}");
   ```
   **Effort:** 2 hours

**Important (Add in Epic 3):**
3. Add malformed CSV error handling test:
   ```csharp
   [Fact]
   public async Task ImportCsv_MalformedRow_OffersSkipOption()
   {
       var csv = "Date,Amount\n2025-01-01,100\nBAD_ROW\n2025-01-02,200";
       var result = await _handler.Handle(new ImportCsvCommand(csv));
       Assert.Equal(2, result.SuccessCount);
       Assert.Equal(1, result.SkippedCount);
   }
   ```
   **Effort:** 3 hours

---

## Maintainability Assessment

**Status:** PASS

### Findings

✅ **PASS Areas:**
- Extensive test data collection (Story 2.1): 20+ CSV formats
- Test fixtures well-organized (standard, edge cases, malformed)
- Unit test coverage in Story 2.2-2.6 ACs
- Integration tests validate 15+ CSV formats (Story 2.3 AC #6)
- VSA structure enforces modularity (ImportCsv feature slice)

✔️ **Strengths:**
- Test-first mindset: Story 2.1 collects data *before* implementation
- Edge cases documented: Multi-line memos, special chars, encoding issues
- Clear success criteria: >90% auto-detection accuracy (Story 2.3)

### Recommendations

**Nice-to-Have:**
- Add CSV format regression test suite (auto-run on every PR)

---

## Critical Issues (Must-Fix Before Release)

### 1. CSV Injection Vulnerability (RCE)
**Severity:** CRITICAL
**Issue:** Malicious formulas in CSV cells could execute on export
**Fix:** Sanitize cells starting with `=+-@|` in Story 2.6
**Effort:** 3 hours
**Test:**
```csharp
[Theory]
[InlineData("=cmd|'/c calc'", "'=cmd|'/c calc'")]
[InlineData("+SUM(A1:A10)", "'+SUM(A1:A10)")]
public void SanitizeCsvCell_FormulaPrefixes_EscapedCorrectly(string input, string expected)
{
    Assert.Equal(expected, _sanitizer.SanitizeCsvCell(input));
}
```

### 2. No Performance Test for NFR2
**Severity:** HIGH
**Issue:** 1,000 txn import time not validated in Epic 2
**Fix:** Add test in Story 2.6 (code above)
**Effort:** 4 hours

### 3. Partial Import Failure → Data Inconsistency
**Severity:** HIGH
**Issue:** No rollback strategy for mid-import failures
**Fix:** Wrap import in database transaction (code above)
**Effort:** 4 hours

### 4. Duplicate Detection O(n²) Complexity
**Severity:** HIGH
**Issue:** Could exceed 5s target for 1,000 txns × 5,000 existing
**Fix:** Use hash dictionary lookup (code above)
**Effort:** 2 hours

---

## Quick Wins (Low Effort, High Impact)

### 1. Add File Size Limit
**Effort:** 30 minutes
**Impact:** Prevent DoS via massive file uploads
**Code:** (see Security section above)

### 2. Path Traversal Protection
**Effort:** 1 hour
**Impact:** Prevent file system access attacks
**Code:** (see Security section above)

### 3. Preload ImportRules Dictionary
**Effort:** 1 hour
**Impact:** Reduce import time by 2-3s
**Code:** (see Performance section above)

---

## NFR Traceability Matrix

| NFR | Requirement | Epic 2 Coverage | Evidence | Status |
|-----|-------------|----------------|----------|--------|
| NFR2 | CSV import <5s for 1,000 txns | ⚠️ **NOT TESTED** | Story 2.2 AC #5 shows progress, but no perf test | **MISSING** |
| NFR5 | 60fps UI interactions | ⚠️ At Risk | Sync CSV parsing could freeze UI | **CONCERNS** |
| NFR8 | Atomic writes + backups | ✅ Inherited | Epic 1's atomic write reused | PASS |
| NFR9 | >95% CSV import success | ✅ Testable | Story 2.3 AC #6 validates 15+ formats (target >90%) | PASS |
| NFR10 | <0.1% crash rate | ⚠️ Partial | Malformed CSV handling incomplete (Story 2.2 AC #4) | **CONCERNS** |
| NFR11 | Zero data loss | ⚠️ At Risk | No rollback for partial import failures | **CONCERNS** |
| NFR14 | 100% hledger validation | ✅ Inherited | Epic 1's validation enforced | PASS |

---

## Gate-Ready YAML Block

```yaml
nfr_validation:
  _assessed: [security, performance, reliability, maintainability]
  _epic: "Epic 2: Import CSV Vertical Slice"
  _date: "2025-10-05"
  _risk_level: HIGH

  security:
    status: CONCERNS
    notes: |
      CRITICAL:
      - CSV injection (RCE via formulas) - MUST FIX
      - Path traversal in file upload - MUST FIX
      HIGH:
      - Unlimited file size (DoS) - MUST FIX
      MEDIUM:
      - Encoding injection edge cases - Add tests
      Action: Add sanitization + validation before Story 2.6 completion

  performance:
    status: CONCERNS
    notes: |
      CRITICAL:
      - No performance test for NFR2 (<5s for 1,000 txns) - MUST ADD
      HIGH:
      - Duplicate detection O(n²) complexity - Use hash index
      - CSV parsing blocks UI - Use async streaming
      MEDIUM:
      - Category suggestion N+1 queries - Preload dictionary
      Action: Add perf test + optimizations to Story 2.6

  reliability:
    status: CONCERNS
    notes: |
      HIGH:
      - No rollback for partial import failures - Add transaction wrapper
      MEDIUM:
      - Hash collision false positives - Improve hash uniqueness
      - Malformed CSV recovery incomplete - Add skip/abort options
      LOW:
      - No "undo import" feature - Defer to Phase 2
      Action: Add transactional import to Story 2.6

  maintainability:
    status: PASS
    notes: |
      - Excellent test data collection (20+ CSV formats)
      - Clear success criteria (>90% auto-detection)
      - Integration tests validate real-world formats
      - VSA structure enforces modularity
      Recommendation: Add CSV regression test suite
```

---

## Risk-Based Testing Strategy

### High-Risk Areas (Focus 60% of Test Effort)

1. **CSV Injection** (Security)
   - Test: Formulas in payee, memo, amount fields
   - Test: Export sanitized CSV → open in Excel → verify no execution
   - Coverage: 100% of injection vectors

2. **Performance Under Load** (Performance)
   - Test: 1,000 txns in <5s (NFR2)
   - Test: 10,000 existing txns + 1,000 import with duplicate detection
   - Coverage: P95, P99 latency measurements

3. **Partial Import Failures** (Reliability)
   - Test: Simulate DB failure at txn 500/1000 → verify rollback
   - Test: Simulate disk full during .hledger write → verify .bak restore
   - Coverage: All failure scenarios

### Medium-Risk Areas (Focus 30% of Test Effort)

4. **Malformed CSV Handling** (Reliability)
   - Test: Invalid dates, non-numeric amounts, encoding errors
   - Test: 10+ edge cases from Story 2.1 collection
   - Coverage: All documented edge cases

5. **Encoding Attacks** (Security)
   - Test: NULL bytes, BOMs, homoglyphs in payee names
   - Coverage: UTF-8, ISO-8859-1, Windows-1252

### Low-Risk Areas (Focus 10% of Test Effort)

6. **UI Interactions** (UX)
   - Test: Drag-drop vs file picker
   - Test: Progress indicator visibility
   - Coverage: Happy path only

---

## Architecture Recommendations

### Refactoring Suggestions (Post-MVP)

1. **Extract CSV Parser to Separate Service**
   - Current: ImportCsvCommand handler does everything
   - Better: Separate concerns (parse, map, validate, import)
   - Benefit: Easier to add new import formats (OFX, QIF)

2. **Add Circuit Breaker for hledger Calls**
   - Current: No protection against hledger process hangs
   - Better: Timeout + circuit breaker (fail fast after 3 timeouts)
   - Benefit: Improves reliability under stress

---

**Assessment Complete**
**Next Step:** Address 4 critical issues + add performance test before Story 2.6 completion

**Estimated Fix Effort:** 17.5 hours
**Recommended Timeline:** Add 1 week to Epic 2 (Week 2-3 → Week 2-4)
